# hadoop-tutorial


<br/>
## 1 - hadoop-tutorial general presentation

This project purpose is to present and learn about the basic features of Hadoop and MapReduce jobs and how it can be used with higher level frameworks such as Kafka, HBase, Avro, Spack, ...

The dataset used here will be a simulation of a stock exchange : agents, orders, prices, ...
[ATOM](http://atom.univ-lille1.fr/) (ArTificial Open Market) is used to generate the dataset.


<br/>
### 1.1 - How does [ATOM](http://atom.univ-lille1.fr/) work ?


#### 1.1.1 - ATOM general presentation
ATOM is a general environment for agent-based simulations of stock markets.
This simulator is able to closely reproduce the behaviour of most order-driven markets and is more specifically tailored for the NYSE Euronext stock exchange.
ATOM is able to efficiently generate, play or replay order flows for intra and extra-days and can be used to design experiments mixing human beings and artificial traders.

#### 1.1.2 - ATOM detailed presentation 
ATOM is an implementation of an order driven financial market like EuroNEXT/NYSE. These markets rely on the exchange of orders that are automatically matched against each others through double auction books. Thus, an order-driven market involves traders that send orders to a market, a market that gathers several double auction orderbooks, one per asset traded on the market. Since the advent and fast development of electronic markets, order-driven markets have offered more and more orders types even if the most current are market orders (buy/sell at the current price), limit order (buy/sell at a given price) and cancel order (to retract an order that has not yet been matched).




#### 1.1.3 - Order-driven markets concepts in ATOM

<table>
	<tr>
		<th>Entity</th>
		<th>Role</th>
	</tr>
	<tr>
		<td> MarketPlace </td>
		<td>Set of OrderBook. <br/> Each agent must be listed in a MarketPlace to be able to trade. Usually in ATOM, you have one MarketPlace containing several OrderBook and Agent.</td>
	</tr>
	
	<tr>
		<td>OrderBook</td>
		<td>
			Object that receive orders and fix prices.<br/>
			Each time a human being or an artificial agent sends an order, the OrderBook records it and try to execute it. This latter operation delivers a price. If the new order cannot be matched with pending orders it is kept "as is", waiting for a counterpart. In ATOM you can have several OrderBook running simultaneously, which allows to build arbitragist agents, speculators who seek to take advantage of price differences on various assets.
		</td>
	</tr>
	
	<tr>
		<td>Order</td>
		<td>
			Represent an offer to sell or buy an asset. <br/>
			In ATOM several kind of orders can be used: LimitOrders, MarketOrders, CancelOrders, UpdateOrders and many others (see <a href="http://atom.univ-lille1.fr/ruleBook.pdf">EURONEXT RULE BOOK 2009</a> for a detailed description of each order). Agents, both artificial or human beings, can send orders to a MarketPlace. To emit an order, one just have to use the appropriate order class and fulfil the associate constructor.
		</td>
	</tr>
	
	<tr>
		<td>Agent</td>
		<td>
			Represent an entity that sends orders to any OrderBook of the MarketPlace. <br/>
			Agent can receive a behaviour or not, depending upon the level of sophistication the experimenter wants to have in his system. If the agent does not have any behaviour, it will just be able to send orders that have been defined elsewhere in the system (it could simply be a real-world order flow for example). If the Agent have a behaviour, it will be able to manage itself the orders, send them to the MarketPlace and let these orders evolve in the simulation. In that last case (agents gifted with a specific behaviour), frequently, the market will ask all agents whether they want to send an order or not.
		</td>
	</tr>
	
	
	
	<tr>
		<td>Exec</td>
		<td>Represent a fully executed order : buyer and seller are in agreement and the transaction is completed</td>
	</tr>
	
	<tr>
		<td>Price</td>
		<td>Price generated by two orders with the same price for the same share. Here, the quantity can be different in the two orders : only the price matters</td>
	</tr>
	
	<tr>
		<td>Day</td>
		<td>Represent a day in a stock exchange</td>
	</tr>
	
	<tr>
		<td>Tick</td>
		<td>A Tick represents a speacking slot : Agents can place order only when a Tick appears and a day has several Ticks. Agents can also place order before the begenning of the day and just after it's end.</td>
	</tr>	
</table>


#### 1.1.4 - Understand the ATOM simulation parameters

The simulation has many parameters that can be set in the `properties.txt` file :

<table>
	<tr>
		<th>Parameter's name</th>
		<th>Parameter's role</th>
	</tr>
	<tr>
		<td>simul.days</td>
		<td>Number of days in the simulation</td>
	</tr>
	<tr>
		<td>simul.tick.opening</td>
		<td>Number of Ticks before the beginning of the day</td>
	</tr>
	<tr>
		<td>simul.tick.continuous</td>
		<td>Number of Ticks during the day</td>
	</tr>
	<tr>
		<td>simul.tick.closing</td>
		<td>Number of Ticks after the end of the day<</td>
	</tr>
	<tr>
		<td>simul.time.startdate</td>
		<td>The start date of the simulation</td>
	</tr>
	<tr>
		<td>simul.time.openhour</td>
		<td>The market's opening hour</td>
	</tr>
	<tr>
		<td>simul.time.closehour</td>
		<td>The market's closing hour</td>
	</tr>
	<tr>
		<td>simul.agent.cash</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>simul.agent.minprice</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>simul.agent.maxprice</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>simul.agent.minquantity</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>simul.agent.maxquantity</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>atom.orderbooks</td>
		<td>= DOW30  -  [help] Value signification ?</td>
	</tr>
	<tr>
		<td>atom.orderbooks.random</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>atom.agents</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>atom.agents.random</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>atom.marketmaker</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>atom.marketmaker.quantity</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>symbols.orderbooks.DOW30</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>symbols.orderbooks.test</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>symbols.orderbooks.test2</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>symbols.agents.basic</td>
		<td>[help] Value signification ?</td>
	</tr>
	<tr>
		<td>symbols.agents.test2</td>
		<td>[help] Value signification ?</td>
</table>




<br/><br/>
### 1.2 - hadoop-tutorial : what happens under the hood

In this tutorial, we will use ATOM to generate market data, store it in HDFS (Hadoop Distributed File System) and execute map reduce jobs on it. [help] est-ce vrai où y a t'il d'autres choses qu'on fait que je n'ai pas encore vu

Once the project is built (see below), you will use `./atom-generate.sh` command to generate and possibly store directly the simulation data in HDFS.

There is different ways to generate and store the simulation data : manually, with HBase, Kafka or Avro.
To choose **one or several methods** [help] [do we have to choose only one or can we do all methods at the same time ???], you can run the command with flag options or edit `properties.txt` files : 

```
simul.output.file=false 	-- set it to true to choose this output | equivalent to -file flag
simul.output.kafka=false	-- set it to true to choose this output | equivalent to -kafka flag
simul.output.hbase=false	-- set it to true to choose this output | equivalent to -hbase flag
simul.output.avro=false		-- set it to true to choose this output | equivalent to -avro flag
```


**Generate and store data manually :**
The aplication will generate data and store it locally in text files.
You will then have to execute a shell command to copy the data in the HDFS storage.
And finally, you'll be able tu run mar reduce jobs on the data.


**Using HBase :**
[help] I do not know how it works (yet). Help appreciated :)


**Using Kafka :**
[help] I do not know how it works (yet). Help appreciated :)


**Using Avro :**
[help] I do not know how it works (yet). Help appreciated :)


**Remark** : this four methods will be explained with more details below, in the Run section of this page.










<br/><br/>
***
***
<br/>
## 2 - Build

### 2.1 - Prerequisites

Only Maven and Java are required to build the project : [help] is this true ?

* Maven 3.x
* Java jdk // [help] : jdk min version ? jre ?  Is this [link](http://wiki.apache.org/hadoop/HadoopJavaVersions) OK ?
* ATOM (see below)



<br/>
### 2.2 - Include ATOM source files

**Manually**
Add atom.jar to the project directory once you've pulled it from github.
You can download it [here](http://atom.univ-lille1.fr/atom.zip) : unzip the archive and move the atom.jar to the project directory.


**Automatic include** (not working at the moment, use manual method described just above)
~~Patch your JDK~~
~~Download the certificat: https://alm.finaxys.com/ALMsite/cacert.alm.finaxys.com.cer~~
~~Add the previous certificat downloaded to your java trust store.~~

```ruby
# sudo keytool -importcert -file cacert.alm.finaxys.com.cer -keystore $JAVA_HOME/jre/lib/security/cacerts -trustcacerts
```


<br/>
### 2.3 Compile and Build the project
Run the command in the project directory.

    mvn clean compile package




<br/><br/>
***
***
<br/>
## 3 - Run the project


### 3.1 - Prerequisites

**Important** : For now, this "how to" only applies to local installations of the tools. Support and explanations for Horton SandBox are comming soon.


**Install Hadoop**

* On your system : [Linux](https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/SingleCluster.html), [OSX](https://amodernstory.com/2014/09/23/installing-hadoop-on-mac-osx-yosemite/)
* With the [HortonWorks SandBox](http://hortonworks.com/products/sandbox/)

Later on, depending on what you choose to use to generate and store data, you will need to install tools and frameworks such as [HBase](https://hbase.apache.org/), [Zookeeper](https://zookeeper.apache.org/), [Kafka](http://kafka.apache.org/), [Avro](https://avro.apache.org/), [Spark](http://spark.apache.org/), ...


**Update `properties.txt` to set the path to the config files for Hadoop**

* hadoop.conf.core : the conf file needed is core-site.xml
* hadoop.conf.hdfs : the conf file needed is hdfs-site.xml

**You can Update log4j2.xml to change output directory and rolling file strategy**

* By default, files will be written in ../AtomLogs/ directory
* [help] A quoi sert reelement ce fichier ? Est-ce qu'il ne sert que pour le choix d'output -file ou bien aussi pour les autres choix (hbase, kafka et avro) ?




<br/><br/>
### 3.2 - Run the application : hands on !!

```
$HADOOP_HOME is an environment variable referring to the Hadoop install directory.
```

**If Hadoop has not been setup and started**

	$HADOOP_HOME/bin/hdfs namenode -format
	$HADOOP_HOME/sbin/start-dfs.sh
	
**To check that hadoop is up and running**

	-- Web Interface
	http://localhost:50070
	
	-- With the Jps command in your terminal : DataNode, NameNode and SecondaryNameNode must appear in the output
	6262 DataNode
	6366 SecondaryNameNode
	6182 NameNode
	3073 
	6802 Jps

**Start YARN** [help] YARN doit être activé pour MapReduce c'est ça ? Du coup est-ce que le fait d'activer Yarn a quelque chose à voir avec le choix d'output (file, hbase, kafka, avro) ou bien est-ce complètement indépendant ?

    $HADOOP_HOME/sbin/start-yarn.sh
    
**To check that YARN is up and running**

	-- Web Interface
	http://localhost:8088/cluster
	-- With the Jps command in your terminal : ResourceManager and NodeManager must appear
	-- 	in the output
	6680 ResourceManager
	6762 NodeManager

<br/>
#### 3.2.1 - Generate and store data manually

##### Generate Atom files simulation into AtomLogs/

* Update properties.txt to set your output choice to local file
	* `simul.output.file=true`
* Run the command (from the project directory) that generates data
	* `./atom-generate.sh`

##### Copy generated data in HDFS

* Create working directories on HDFS
	* `$HADOOP_HOME/bin/hdfs dfs -mkdir /user/`
	* `$HADOOP_HOME/bin/hdfs dfs -mkdir /user/pierre`
	* `$HADOOP_HOME/bin/hdfs dfs -mkdir /user/pierre/input`
* Put Atom logs files on HDFS
	* `$HADOOP_HOME/bin/hdfs dfs -put AtomLogs/* /user/pierre/input/`

##### Small exercice :

* Look through admin interface created files in HDFS
* check allocated bloc size
* Now try to find the files in the local file size and compare file size
* [help] Je comprends pas trop ici : le but est-il simplement de regarder la différence de taille des fichiers entre les fichiers locaux qui sont dans ../AtomLogs/ et les fichiers copiés dans HDFS ??



#### 3.2.2 - Generate and store data with HBase
[help] : j'ai essayé mais ait eu quelques problèmes, notemment avec zookeeper que je pense lié à ca : 
```
HBase problem
hbase(main):002:0> status
2016-07-12 12:15:43,824 ERROR [main] client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in 'zookeeper.znode.parent'. There could be a mismatch with the one configured in the master.
```
[help] : quelle différence dans le cycle de vie des data entre utiliser HBase et copier manuellement les données dans HDFS ? (mis à par le fait que HBase ajoute une couche de NoSQL à Hadoop)

#### 3.2.3 - Generate and store data with Kafka
[help] : qu'est ce qu'il se passe quand on utilise Kafka ? Est ce qu'on store les data ?


#### 3.2.4 - Generate and store data with Avro
[help] : qu'est ce qu'il se passe quand on utilise Avro ? 


<br/>
#### 3.2.X - Run MapReduce jobs

##### Execute map reduce jobs on cluster TraceCount

```
-- Execute command (processus will be logging information on itself on the terminal)
$HADOOP_HOME/bin/hadoop jar target/hadoop-tutorial-0.2-SNAPSHOT.jar TraceCount /user/pierre/input /user/pierre/tracecount

-- Display result
$HADOOP_HOME/bin/hdfs dfs -cat /user/pierre/tracecount/part-r-00000
```

##### Calculate agent position per day [help] : c'est quoi l'agent position ??

```
-- Execute command
$HADOOP_HOME/bin/hadoop jar target/hadoop-tutorial-0.2-SNAPSHOT.jar AgentPosition /user/pierre/input/ /user/pierre/agentposition

-- Display result
$HADOOP_HOME/bin/hdfs dfs -cat /user/pierre/agentposition/part-r-00000
```







<br/>
#### 3.2.5 - MapReduce exercices

##### Exercice 1
Implement a  MapReduce function which calculate the volume of order for each order book

##### Calculate agent position per day (Join example) [help] : c'est juste après l'exercice 1 mais à quoi ça sert ? A donner un exemple de JOIN dans MapReduce ?

```
-- Execute command
$HADOOP_HOME/bin/hadoop jar target/hadoop-tutorial-0.2-SNAPSHOT.jar LeftOrder /user/pierre/input/ /user/pierre/leftorder
-- Display result
$HADOOP_HOME/bin/hdfs dfs -cat /user/pierre/leftorder/part-r-00000
```

##### Exercice 2
Implement a  MapReduce function which calculate the volume of executed order for each order book






	

<br/><br/>
### 3.3 - Stop the application

**Clean Data**

```
$HADOOP_HOME/bin/hdfs dfs -rm -r /user/pierre/agentposition
$HADOOP_HOME/bin/hdfs dfs -rm -r /user/pierre/tracecount
$HADOOP_HOME/bin/hdfs dfs -rm -r /user/pierre/leftorder
```
    
**Stop YARN and HDFS**

```
$HADOOP_HOME/sbin/stop-yarn.sh
$HADOOP_HOME/sbin/stop-dfs.sh
```




[help] : d'après ce que j'ai compris on a une phase génération de données et stockage et une phase de lancement de calculs map reduce. Ces deux phases sont séparées quand on génère et stocke les données à la main mais est-ce aussi le cas quand on utilise les options hbase, kafka, avro ?
Je ne comprend pas le "workflow" de ces trois options là (hbase, kafka, avro)




<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>
## Other topics (temporary section)

### Configuring Hbase (specifically zookeeper) if connection is refused

Here is the error that might occur : 

```
zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
```

To solve this, you need to add a property to your Hbase configuration file.
The file to edit is hbase-site.xml in your installation folder and you need to add this property :

```xml
<property>
    <name>hbase.zookeeper.quorum</name>
    <value>your_hostname.local</value>
</property>
```

Replace "your_hostname" by your real host name (type hostname in a terminal to have it).

More information [here](http://stackoverflow.com/questions/10188889/hbase-connection-refused).

	

    

    

    
### ALM links

* Sonar - https://alm.finaxys.com:9000/
* Nexus -https://alm.finaxys.com:44313/nexus/index.html#welcome
* Jenkins - https://alm.finaxys.com:44312/job/hadoop-tutorial/





<br/><br/>
## Known issues and solutions

### Execute map reduce jobs on cluster TraceCount

#### Invalid signature file digest for Manifest main attributes error
Error looks like this :

```
Exception in thread "main" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes
	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:284)
	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:238)
	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316)
	at java.util.jar.JarVerifier.update(JarVerifier.java:228)
	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383)
	at java.util.jar.JarFile.getInputStream(JarFile.java:450)
	at org.apache.hadoop.util.RunJar.unJar(RunJar.java:101)
	at org.apache.hadoop.util.RunJar.unJar(RunJar.java:81)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:209)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
```

See issue #7 on hithub here : https://github.com/Finaxys/hadoop-tutorial/issues/7